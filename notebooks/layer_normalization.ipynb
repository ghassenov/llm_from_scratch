{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e922354e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ghass\\.virtualenvs\\llm_from_scratch-Nm31MAtF\\lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f30b5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        # a simple placeholder\n",
    "    def forward(self,x):\n",
    "        # placeholder code\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a535707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2,5)\n",
    "layer = nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcada764",
   "metadata": {},
   "source": [
    "Before applying layer normalization to these outputs, let's examine the mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097b19f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Var\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1,keepdim=True)\n",
    "var = out.var(dim=-1,keepdim=True)\n",
    "print('Mean\\n',mean)\n",
    "print('Var\\n',var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0561d97",
   "metadata": {},
   "source": [
    "Note: using keepdim = True in operations like mean or variance ensures that the output tensor retains the same number of dimensions as the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee0c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean\n",
      " tensor([[9.9341e-09],\n",
      "        [0.0000e+00]], grad_fn=<MeanBackward1>)\n",
      "Variance\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out-mean)/torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1,keepdim=True)\n",
    "var = out_norm.var(dim=-1,keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\",out_norm)\n",
    "print(\"Mean\\n\",mean)\n",
    "print(\"Variance\\n\",var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4e886",
   "metadata": {},
   "source": [
    "To improve readability, we can also turn off the scientific notation when printing tensor values by setting sci_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "601e513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print('Mean:\\n',mean)\n",
    "print('Variance:\\n',var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b235541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self,emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim=-1,keepdim=True)\n",
    "        var = x.var(dim=-1,keepdim=True,unbiased=False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
    "        return self.scale*norm_x + self.shift\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420a88c",
   "metadata": {},
   "source": [
    "This specific implementation of layer Normalization operates on the last dimension of input tensor x,which represents the columns\n",
    "* we add a small constant epsilon to the variance to prevent division by zero during noramlization\n",
    "* The scale and shift are two trainable parameters (of the same dimension as the input) that the llm automatically adjusts during training. This would improve the model's performance on its training task.\n",
    "* This allows the model to learn appropriate scaling and shifting that best suit the data it is processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812fe5a",
   "metadata": {},
   "source": [
    "Note on biased variance :\n",
    "* In our variance calculation method, we have opted for an implementation detail by setting unbiased=False\n",
    "* In the variance calculation, we divide by the number of inputs n in the variance formula. This approach does not appy Bessel's correction, which typically uses n-1 instead of n in the denominator to adjust for bias in sample variance estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d6625b",
   "metadata": {},
   "source": [
    "Let's Now put our new layer into practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da04d9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Var:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1,keepdim=True)\n",
    "var = out_ln.var(dim=-1,unbiased=False,keepdim=True)\n",
    "print(\"Mean\\n\",mean)\n",
    "print(\"Var:\\n\",var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch-Nm31MAtF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
