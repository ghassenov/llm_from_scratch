{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e9b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b7b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from modular.GPT_architecture.FeedForwardBlock import GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f941209",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self,layer_sizes,use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0],layer_sizes[1]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1],layer_sizes[2]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2],layer_sizes[3]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3],layer_sizes[4]),GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4],layer_sizes[5]),GELU()),\n",
    "        ])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        for layer in self.layers:\n",
    "            # compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x+ layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27833f93",
   "metadata": {},
   "source": [
    "This code implements a deep neural network with 5 layers, each consisting of a linear layer and a GeLU activation function\n",
    "* In the forward pass, we iteratively pass the input through the layers and optionally add the shortcut connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d95658",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1.,0.,-1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes,use_shortcut=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "871a78b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model,x):\n",
    "    #forward pass\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    \n",
    "    #calculate loss based on how close the target and output are\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output,target)\n",
    "    \n",
    "    #backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "    for name,param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # print the mean absolute gradient of the weights\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0a2d2",
   "metadata": {},
   "source": [
    "* we specify a loss function that computes how close the model output and a user-specified target\n",
    "* Then, when calling loss.backward(), PyTorch computes the loss gradient for each layer in the model\n",
    "* we can iterate through the weight parameters via model.named_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e50fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466\n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182\n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078\n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut,sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c046e544",
   "metadata": {},
   "source": [
    "Let's now instantiate a model with skip connections and see how it compares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9934c956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.2216978669166565\n",
      "layers.1.0.weight has gradient mean of 0.20694100856781006\n",
      "layers.2.0.weight has gradient mean of 0.3289698660373688\n",
      "layers.3.0.weight has gradient mean of 0.2665731906890869\n",
      "layers.4.0.weight has gradient mean of 1.3258538246154785\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes,use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut,sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f0126f",
   "metadata": {},
   "source": [
    "As we can see, based on the output, the last layer (layers.4) still has a larger gradient than the other layers.\n",
    "* However, the gradient value stabilizes as we progress towards the first layer (lyers.0) and doesn't shrink to a vanishly small value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_from_scratch-Nm31MAtF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
